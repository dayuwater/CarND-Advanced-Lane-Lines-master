{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detecting Lanes and Cars in a Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import numpy as np\n",
    "import time\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from skimage.feature import hog\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from Line import *\n",
    "from lesson_functions import *\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Camera Calibration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:05<00:00,  3.23it/s]\n"
     ]
    }
   ],
   "source": [
    "# preload function to get nessesary global variables\n",
    "cameraMatrix, calibrate = calculate_camera_distortion('camera_cal/calibration*.jpg')\n",
    "M, Minv = calculate_perspective_transform()\n",
    "left_lane = Line()\n",
    "right_lane = Line()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lane Detection Classes and Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define a class to receive the characteristics of each line detection\n",
    "class Line():\n",
    "    def __init__(self):\n",
    "        # was the line detected in the last iteration?\n",
    "        self.detected = False  \n",
    "        # x values of the last n fits of the line\n",
    "        self.recent_xfitted = [] \n",
    "        #average x values of the fitted line over the last n iterations\n",
    "        self.bestx = None     \n",
    "        #polynomial coefficients averaged over the last n iterations\n",
    "        self.best_fit = None  \n",
    "        #polynomial coefficients for the most recent fit\n",
    "        self.current_fit = [np.array([False])]  \n",
    "        #radius of curvature of the line in some units\n",
    "        self.radius_of_curvature = None \n",
    "        #distance in meters of vehicle center from the line\n",
    "        self.line_base_pos = None \n",
    "        #difference in fit coefficients between last and new fits\n",
    "        self.diffs = np.array([0,0,0], dtype='float') \n",
    "        #x values for detected line pixels\n",
    "        self.allx = None  \n",
    "        #y values for detected line pixels\n",
    "        self.ally = None\n",
    "\n",
    "def calculate_camera_distortion(images):\n",
    "    # prepare object points, like (0,0,0), (1,0,0), (2,0,0) ....,(6,5,0)\n",
    "    objp = np.zeros((6*9,3), np.float32)\n",
    "    objp[:,:2]= np.mgrid[0:9,0:6].T.reshape(-1,2)\n",
    "\n",
    "\n",
    "\n",
    "    # Arrays to store object points and image points from all the images.\n",
    "    objpoints = []# 3d points in real world space\n",
    "    imgpoints = []# 2d points in image plane.\n",
    "\n",
    "    # Make a list of calibration images\n",
    "    images = glob.glob(images)\n",
    "\n",
    "    test_imgs = []\n",
    "\n",
    "    # camera calibration results\n",
    "    cameraMatrix = None\n",
    "    calibrate = None\n",
    "\n",
    "\n",
    "    # Step through the list and search for chessboard corners\n",
    "    for i, fname in enumerate(tqdm(images)):\n",
    "        img = cv2.imread(fname)\n",
    "        test_imgs.append(img)\n",
    "        gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        # Find the chessboard corners\n",
    "        ret, corners = cv2.findChessboardCorners(gray, (9,6),None)\n",
    "\n",
    "        # If found, add object points, image points\n",
    "        if ret == True:\n",
    "            objpoints.append(objp)\n",
    "            imgpoints.append(corners)\n",
    "\n",
    "    # calculate the camera matrix and calibration parameters using all the corners found in the 20 calibration images\n",
    "    retval, cameraMatrix, calibrate, rvecs, tvecs = cv2.calibrateCamera(objpoints, imgpoints, gray.shape[::-1], None, None)\n",
    "\n",
    "    return cameraMatrix, calibrate\n",
    "\n",
    "def calculate_perspective_transform():\n",
    "    # get the perspective transform matricies\n",
    "    src = np.float32([[220,700],[600,450],[675,450],[1050,700]])\n",
    "    dst = np.float32([[325,700],[325,0],[950,0],[950,700]])\n",
    "    M = cv2.getPerspectiveTransform(src, dst)\n",
    "    Minv = cv2.getPerspectiveTransform(dst, src)\n",
    "    return M, Minv\n",
    "\n",
    "def cal_undistort(img):\n",
    "    gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
    "    undist = cv2.undistort(img, cameraMatrix, calibrate, None, cameraMatrix)\n",
    "    return undist\n",
    "\n",
    "# filter the image using absolute sobel gradient in either x or y axis\n",
    "def abs_sobel_thresh(img, orient='x', sobel_kernel=3, thresh=(0, 255)):\n",
    "    # Convert to grayscale\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "    # Apply x or y gradient with the OpenCV Sobel() function\n",
    "    # and take the absolute value\n",
    "    if orient == 'x':\n",
    "        abs_sobel = np.absolute(cv2.Sobel(gray, cv2.CV_64F, 1, 0))\n",
    "    if orient == 'y':\n",
    "        abs_sobel = np.absolute(cv2.Sobel(gray, cv2.CV_64F, 0, 1))\n",
    "    # Rescale back to 8 bit integer\n",
    "    scaled_sobel = np.uint8(255*abs_sobel/np.max(abs_sobel))\n",
    "    # Create a copy and apply the threshold\n",
    "    binary_output = np.zeros_like(scaled_sobel)\n",
    "    # Here I'm using inclusive (>=, <=) thresholds, but exclusive is ok too\n",
    "    binary_output[(scaled_sobel >= thresh[0]) & (scaled_sobel <= thresh[1])] = 1\n",
    "\n",
    "    # Return the result\n",
    "    return binary_output\n",
    "\n",
    "# filter the image using the magnitude of the sobel gradient\n",
    "def mag_thresh(image, sobel_kernel=3, mag_thresh=(0, 255)):\n",
    "    # Apply the following steps to img\n",
    "    # 1) Convert to grayscale\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
    "    # 2) Take the gradient in x and y separately\n",
    "    sobelx = cv2.Sobel(gray, cv2.CV_64F, 1, 0, ksize = sobel_kernel)\n",
    "    sobely = cv2.Sobel(gray, cv2.CV_64F, 0, 1, ksize = sobel_kernel)\n",
    "    # 3) Calculate the magnitude \n",
    "    mag = np.sqrt(sobelx**2 + sobely**2)\n",
    "    # 4) Scale to 8-bit (0 - 255) and convert to type = np.uint8\n",
    "    scaled_sobel = np.uint8(mag / np.max(mag) * 255)\n",
    "    # 5) Create a binary mask where mag thresholds are met\n",
    "    binary_output = np.zeros_like(scaled_sobel)\n",
    "    binary_output[(scaled_sobel > mag_thresh[0]) & (scaled_sobel < mag_thresh[1])] = 1\n",
    "    # 6) Return this mask as your binary_output image\n",
    "    #binary_output = np.copy(img) # Remove this line\n",
    "    return binary_output\n",
    "\n",
    "# filter the image using the direction of the sobel gradient\n",
    "def dir_threshold(image, sobel_kernel=3, thresh=(0, np.pi/2)):\n",
    "    # Apply the following steps to img\n",
    "    # 1) Convert to grayscale\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
    "    # 2) Take the gradient in x and y separately\n",
    "    sobelx = cv2.Sobel(gray, cv2.CV_64F, 1, 0, ksize = sobel_kernel)\n",
    "    sobely = cv2.Sobel(gray, cv2.CV_64F, 0, 1, ksize = sobel_kernel)\n",
    "    # 3) Take the absolute value of the x and y gradients\n",
    "    abs_sobelx = np.absolute(sobelx)\n",
    "    abs_sobely = np.absolute(sobely)\n",
    "    # 4) Use np.arctan2(abs_sobely, abs_sobelx) to calculate the direction of the gradient\n",
    "    direction = np.arctan2(abs_sobely, abs_sobelx)\n",
    "    # 5) Create a binary mask where direction thresholds are met\n",
    "    binary_output = np.zeros_like(direction)\n",
    "    binary_output[(direction > thresh[0]) & (direction < thresh[1])] = 1\n",
    "    # 6) Return this mask as your binary_output image\n",
    "    #binary_output = np.copy(img) # Remove this line\n",
    "    return binary_output\n",
    "    \n",
    "def gradient_threshold(img):\n",
    "    # Choose a Sobel kernel size\n",
    "    ksize = 3 # Choose a larger odd number to smooth gradient measurements\n",
    "\n",
    "    # Apply each of the thresholding functions\n",
    "    gradx = abs_sobel_thresh(img, orient='x', sobel_kernel=ksize, thresh=(30, 150))\n",
    "    grady = abs_sobel_thresh(img, orient='y', sobel_kernel=ksize, thresh=(30, 150))\n",
    "    mag_binary = mag_thresh(img, sobel_kernel=ksize, mag_thresh=(45, 255))\n",
    "    dir_binary = dir_threshold(img, sobel_kernel=ksize, thresh=(-1.3, 1.3))\n",
    "\n",
    "    combined = np.zeros_like(dir_binary)\n",
    "    # Choosing the pixels that are either chosen by both x and y absolute sobel filter, \n",
    "    # or by both magnitude and directional sobel filters\n",
    "    combined[((gradx == 1) & (grady == 1)) | ((mag_binary == 1) & (dir_binary == 1))] = 1\n",
    "    \n",
    "    return combined\n",
    "\n",
    "def hls_select(img, s_thresh=(0, 255), l_thresh=(0,255), h_thresh=(0,360)):\n",
    "    # 1) Convert to HLS color space\n",
    "    hls = cv2.cvtColor(img, cv2.COLOR_RGB2HLS)\n",
    "    # 2) Apply a threshold to the S channel\n",
    "    s = hls[:,:,2]\n",
    "    l = hls[:,:,1]\n",
    "    h = hls[:,:,0]\n",
    "    binary_output = np.zeros_like(s)\n",
    "    binary_output[(s > s_thresh[0]) & (s <= s_thresh[1]) & (l > l_thresh[0]) & (l <= l_thresh[1]) & (h > h_thresh[0]) & (h <= h_thresh[1])] = 1\n",
    "    # 3) Return a binary image of threshold result\n",
    "    #binary_output = np.copy(img) # placeholder line\n",
    "    return binary_output\n",
    "\n",
    "def color_filters(img):\n",
    "    img = hls_select(img, s_thresh=(90, 255), l_thresh=(40,255), h_thresh=(0,360))\n",
    "    return img\n",
    "\n",
    "def histogram_lanes(img):\n",
    "    histogram = np.sum(img[img.shape[0]//2:,:], axis=0)\n",
    "    plt.plot(histogram)\n",
    "\n",
    "def find_lanes(source_img, img, Minv, reuse = False, left_lane = None, right_lane = None):\n",
    "    '''\n",
    "    Find and calculate the lane curvature, and draw the lane area back to the source image\n",
    "    '''\n",
    "    # TODO: Store the line parameters into the Line class\n",
    "    \n",
    "    \n",
    "    # Assuming you have created a warped binary image called \"img\"\n",
    "    # Take a histogram of the bottom half of the image\n",
    "    histogram = np.sum(img[img.shape[0]//2:,:], axis=0)\n",
    "    # Find the peak of the left and right halves of the histogram\n",
    "    # These will be the starting point for the left and right lines\n",
    "    midpoint = np.int(histogram.shape[0]/2)\n",
    "    leftx_base = np.argmax(histogram[:midpoint])\n",
    "    rightx_base = np.argmax(histogram[midpoint:]) + midpoint\n",
    "\n",
    "    # Choose the number of sliding windows\n",
    "    nwindows = 9\n",
    "    # Set height of windows\n",
    "    window_height = np.int(img.shape[0]/nwindows)\n",
    "    # Identify the x and y positions of all nonzero pixels in the image\n",
    "    nonzero = img.nonzero()\n",
    "    nonzeroy = np.array(nonzero[0])\n",
    "    nonzerox = np.array(nonzero[1])\n",
    "    # Current positions to be updated for each window\n",
    "    leftx_current = leftx_base\n",
    "    rightx_current = rightx_base\n",
    "    # Set the width of the windows +/- margin\n",
    "    margin = 100\n",
    "    # Set minimum number of pixels found to recenter window\n",
    "    minpix = 50\n",
    "    # Create empty lists to receive left and right lane pixel indices\n",
    "    left_lane_inds = []\n",
    "    right_lane_inds = []\n",
    "   \n",
    "    # If we are reusing previous results\n",
    "    if reuse:\n",
    "        if left_lane.detected and right_lane.detected:\n",
    "            left_fit = left_lane.current_fit\n",
    "            right_fit =  right_lane.current_fit\n",
    "            left_lane_inds = ((nonzerox > (left_fit[0]*(nonzeroy**2) + left_fit[1]*nonzeroy + left_fit[2] - margin)) & (nonzerox < (left_fit[0]*(nonzeroy**2) + left_fit[1]*nonzeroy + left_fit[2] + margin))) \n",
    "            right_lane_inds = ((nonzerox > (right_fit[0]*(nonzeroy**2) + right_fit[1]*nonzeroy + right_fit[2] - margin)) & (nonzerox < (right_fit[0]*(nonzeroy**2) + right_fit[1]*nonzeroy + right_fit[2] + margin)))\n",
    "\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        # If we are not reusing previous results, start from scratch\n",
    "        # Step through the windows one by one\n",
    "        for window in range(nwindows):\n",
    "            # Identify window boundaries in x and y (and right and left)\n",
    "            win_y_low = img.shape[0] - (window+1)*window_height\n",
    "            win_y_high = img.shape[0] - window*window_height\n",
    "            win_xleft_low = leftx_current - margin\n",
    "            win_xleft_high = leftx_current + margin\n",
    "            win_xright_low = rightx_current - margin\n",
    "            win_xright_high = rightx_current + margin\n",
    "\n",
    "            # Identify the nonzero pixels in x and y within the window\n",
    "            good_left_inds = ((nonzeroy >= win_y_low) & (nonzeroy < win_y_high) & (nonzerox >= win_xleft_low) & (nonzerox < win_xleft_high)).nonzero()[0]\n",
    "            good_right_inds = ((nonzeroy >= win_y_low) & (nonzeroy < win_y_high) & (nonzerox >= win_xright_low) & (nonzerox < win_xright_high)).nonzero()[0]\n",
    "            # Append these indices to the lists\n",
    "            left_lane_inds.append(good_left_inds)\n",
    "            right_lane_inds.append(good_right_inds)\n",
    "            # If you found > minpix pixels, recenter next window on their mean position\n",
    "            if len(good_left_inds) > minpix:\n",
    "                leftx_current = np.int(np.mean(nonzerox[good_left_inds]))\n",
    "            if len(good_right_inds) > minpix:        \n",
    "                rightx_current = np.int(np.mean(nonzerox[good_right_inds]))\n",
    "\n",
    "        # Concatenate the arrays of indices\n",
    "        left_lane_inds = np.concatenate(left_lane_inds)\n",
    "        right_lane_inds = np.concatenate(right_lane_inds)\n",
    "    \n",
    "    \n",
    "    (left_lane_inds)\n",
    "    (right_lane_inds)\n",
    "    \n",
    "    # Extract left and right line pixel positions\n",
    "    leftx = nonzerox[left_lane_inds]\n",
    "    lefty = nonzeroy[left_lane_inds] \n",
    "    rightx = nonzerox[right_lane_inds]\n",
    "    righty = nonzeroy[right_lane_inds] \n",
    "    # Fit a second order polynomial to each\n",
    "    left_fit = np.polyfit(lefty, leftx, 2)\n",
    "    right_fit = np.polyfit(righty, rightx, 2)\n",
    "    \n",
    "\n",
    "    # Generate x and y values for plotting\n",
    "    ploty = np.linspace(0, img.shape[0]-1, img.shape[0] )\n",
    "    left_fitx = left_fit[0]*ploty**2 + left_fit[1]*ploty + left_fit[2]\n",
    "    right_fitx = right_fit[0]*ploty**2 + right_fit[1]*ploty + right_fit[2]\n",
    "    y_eval = np.max(ploty)\n",
    "    \n",
    "    # Create an image to draw the lines on\n",
    "    warp_zero = np.zeros_like(img).astype(np.uint8)\n",
    "    color_warp = np.dstack((warp_zero, warp_zero, warp_zero)) * 255\n",
    "    color_warp[lefty, leftx] = [255, 0, 0]\n",
    "    color_warp[righty, rightx] = [0, 0, 255]\n",
    "\n",
    "    # Recast the x and y points into usable format for cv2.fillPoly()\n",
    "    pts_left = np.array([np.transpose(np.vstack([left_fitx, ploty]))])\n",
    "    pts_right = np.array([np.flipud(np.transpose(np.vstack([right_fitx, ploty])))])\n",
    "    pts = np.hstack((pts_left, pts_right))\n",
    "\n",
    "    # Draw the lane onto the warped blank image\n",
    "    cv2.fillPoly(color_warp, np.int_([pts]), (0,255, 0))\n",
    "\n",
    "\n",
    "    # Warp the blank back to original image space using inverse perspective matrix (Minv)\n",
    "    newwarp = cv2.warpPerspective(color_warp, Minv, (img.shape[1], img.shape[0])) \n",
    "    # Combine the result with the original image\n",
    "    result = cv2.addWeighted(source_img, 1, newwarp, 0.3, 0)\n",
    "    \n",
    "\n",
    "    \n",
    "    # Define conversions in x and y from pixels space to meters\n",
    "    ym_per_pix = 30/720 # meters per pixel in y dimension\n",
    "    xm_per_pix = 3.7/700 # meters per pixel in x dimension\n",
    "    \n",
    "    # Calculate the new radii of curvature\n",
    "    left_fit_cr = np.polyfit(lefty*ym_per_pix, leftx*xm_per_pix, 2)\n",
    "    right_fit_cr = np.polyfit(righty*ym_per_pix, rightx*xm_per_pix, 2)\n",
    "    \n",
    "    left_curverad = ((1 + (2*left_fit_cr[0]*y_eval*ym_per_pix + left_fit_cr[1])**2)**1.5) / np.absolute(2*left_fit_cr[0])\n",
    "    right_curverad = ((1 + (2*right_fit_cr[0]*y_eval*ym_per_pix + right_fit_cr[1])**2)**1.5) / np.absolute(2*right_fit_cr[0])\n",
    "    # Now our radius of curvature is in meters\n",
    "#     (left_curverad, 'm', right_curverad, 'm')\n",
    "    # negative = right, positive = left\n",
    "    \n",
    "    offset = (leftx_base - midpoint + rightx_base - midpoint) / 2\n",
    "    if offset < 0:\n",
    "        direction = \"right\"\n",
    "    else:\n",
    "        direction = \"left\"\n",
    "    \n",
    "    font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "    cv2.putText(result,'Curvature = {}m'.format(int(min(left_curverad,right_curverad))),(10,50), font, 1,(255,255,255),2)\n",
    "    cv2.putText(result,'Car is {}m {} of center'.format(int(abs(offset * xm_per_pix * 100)) / 100, direction),(10,150), font, 1,(255,255,255),2)\n",
    "    \n",
    "    # store the result into the Line class\n",
    "    if not left_lane.detected:\n",
    "        left_lane.detected = True\n",
    "    if not right_lane.detected:\n",
    "        right_lane.detected = True\n",
    "        \n",
    "    # store the left lane\n",
    "    left_lane.current_fit = left_fit\n",
    "    left_lane.radius_of_curvature = left_curverad\n",
    "    left_lane.line_base_pos = (midpoint - leftx_base) * xm_per_pix\n",
    "    left_lane.allx = leftx\n",
    "    left_lane.ally = lefty\n",
    "    \n",
    "\n",
    "    # store the right lane\n",
    "    right_lane.current_fit = right_fit\n",
    "    right_lane.radius_of_curvature = right_curverad\n",
    "    right_lane.line_base_pos = (rightx_base - midpoint) * xm_per_pix\n",
    "    right_lane.allx = rightx\n",
    "    right_lane.ally = righty\n",
    "    \n",
    "    \n",
    "    return result, left_lane, right_lane\n",
    "\n",
    "def preprocess(img, M):\n",
    "    '''\n",
    "    Convert the source image to perspective transformed binary lane maps\n",
    "    '''\n",
    "    test_img2 = cal_undistort(img.copy()) # need to apply step 2 here\n",
    "    gradient_thresholded = gradient_threshold(test_img2)\n",
    "    color_filtered = color_filters(test_img2)\n",
    "    combined_binary = np.zeros_like(gradient_thresholded)\n",
    "    combined_binary[(gradient_thresholded == 1) | (color_filtered == 1)] = 1\n",
    "    binary_warped = cv2.warpPerspective(combined_binary, M, (img.shape[1], img.shape[0]))\n",
    "    return binary_warped\n",
    "\n",
    "def detect_lanes_in_image(img, reuse = True):\n",
    "    cv2.imwrite(\"101.jpg\",img)\n",
    "    if img.shape != (720, 1280, 3):\n",
    "        img = cv2.resize(img, (1280, 720))\n",
    "    after_preprocess = preprocess(img,M)\n",
    "    return find_lanes(img, after_preprocess, Minv, reuse = reuse, left_lane = left_lane, right_lane = right_lane)[0]\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detect Lanes in Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# must keep this block of code for some unexplained reason\n",
    "# This block processes individual images, not videos\n",
    "import os\n",
    "# get the perspective transform matricies\n",
    "src = np.float32([[220,700],[600,450],[675,450],[1050,700]])\n",
    "dst = np.float32([[325,700],[325,0],[950,0],[950,700]])\n",
    "M = cv2.getPerspectiveTransform(src, dst)\n",
    "Minv = cv2.getPerspectiveTransform(dst, src)\n",
    "# load image   \n",
    "image_paths = os.listdir(\"test_images\")\n",
    "in_images = []\n",
    "out_images = []\n",
    "left_lane = Line()\n",
    "right_lane = Line()\n",
    "images = glob.glob('test_images/*.jpg')\n",
    "images.extend(glob.glob('test_images/*.png'))\n",
    "for img_path in images:\n",
    "    img = cv2.imread(img_path)\n",
    "    file_path = img_path.split(\"/\")[1]\n",
    "    in_images.append((file_path,img))\n",
    "    # This is the crucial line\n",
    "    out_img = process_image(img, reuse = False)\n",
    "    cv2.imwrite(\"output_images/\"+file_path,out_img)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classes and Functions for Car Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_hog_features(img, orient, pix_per_cell, cell_per_block, vis=False, feature_vec=True):\n",
    "    if vis == True:\n",
    "        features, hog_image = hog(img, orientations=orient, pixels_per_cell=(pix_per_cell, pix_per_cell),\n",
    "                                  cells_per_block=(cell_per_block, cell_per_block), transform_sqrt=False, \n",
    "                                  visualise=True, feature_vector=False)\n",
    "        return features, hog_image\n",
    "    else:      \n",
    "        features = hog(img, orientations=orient, pixels_per_cell=(pix_per_cell, pix_per_cell),\n",
    "                       cells_per_block=(cell_per_block, cell_per_block), transform_sqrt=False, \n",
    "                       visualise=False, feature_vector=feature_vec)\n",
    "        return features\n",
    "    \n",
    "def draw_labeled_bboxes(img, labels):\n",
    "    # Iterate through all detected cars\n",
    "    for car_number in range(1, labels[1]+1):\n",
    "        # Find pixels with each car_number label value\n",
    "        nonzero = (labels[0] == car_number).nonzero()\n",
    "        # Identify x and y values of those pixels\n",
    "        nonzeroy = np.array(nonzero[0])\n",
    "        nonzerox = np.array(nonzero[1])\n",
    "        # Define a bounding box based on min/max x and y\n",
    "        bbox = ((np.min(nonzerox), np.min(nonzeroy)), (np.max(nonzerox), np.max(nonzeroy)))\n",
    "        # Draw the box on the image\n",
    "        cv2.rectangle(img, bbox[0], bbox[1], (0,0,255), 6)\n",
    "    # Return the image\n",
    "    return img\n",
    "\n",
    "class BoundingBoxes:\n",
    "    def __init__(self, max_frame = 6):\n",
    "        self.bboxes = []  # a list of recently discovered bounding boxes. This should be 2D\n",
    "        self.max_frame = max_frame # maximum frames of bboxes to store\n",
    "        \n",
    "    def add_bboxes(self, bboxes):\n",
    "        # if there are less than the maximum frames stored, add in\n",
    "        if len(self.bboxes) < self.max_frame:\n",
    "            self.bboxes.append(bboxes)\n",
    "        # else, pop the first one\n",
    "        else:\n",
    "            self.bboxes.pop(0)\n",
    "            self.bboxes.append(bboxes)\n",
    "            \n",
    "from scipy.ndimage.measurements import label\n",
    "import itertools\n",
    "def find_cars_in_image(img, bb = None):\n",
    "    # perhaps this is not correct for MPG images\n",
    "    \n",
    "    img = img.astype(np.float32) / 255\n",
    "    draw_img = np.copy(img)\n",
    "    \n",
    "    \n",
    "    # doing multiple window search\n",
    "    # because the car is at the most left lane for all the test images and videos, \n",
    "    # it is easy to eliminate some of the false positives just by cutting off all the left region\n",
    "    # However, this will not be a valid solution if the car is not at the left\n",
    "    \n",
    "    # first, add search window of size 64 in order to search farther cars \n",
    "    windows_far = slide_window(img, x_start_stop=[400, None], y_start_stop=[400,500], \n",
    "                    xy_window=(64, 64), xy_overlap=(0.7, 0.7))\n",
    "    # then, add search window of size 80 in order to search closer cars\n",
    "    windows_close = slide_window(img, x_start_stop=[400, None], y_start_stop=[400,500], \n",
    "                    xy_window=(80, 80), xy_overlap=(0.8, 0.8))\n",
    "    \n",
    "    windows = [*windows_far, *windows_close]\n",
    "\n",
    "    hot_windows = search_windows(img, windows, svc, X_scaler, color_space=color_space, \n",
    "                            spatial_size=spatial_size, hist_bins=hist_bins, \n",
    "                            orient=orient, pix_per_cell=pix_per_cell, \n",
    "                            cell_per_block=cell_per_block, \n",
    "                            hog_channel=hog_channel, spatial_feat=spatial_feat, \n",
    "                            hist_feat=hist_feat, hog_feat=hog_feat)  \n",
    "    \n",
    "    # add previous detections\n",
    "    this_detection = hot_windows\n",
    "    if bb is not None:\n",
    "        hot_windows = [*hot_windows, *list(itertools.chain(*bb.bboxes))]\n",
    "    \n",
    "    # print(windows)\n",
    "\n",
    "    window_img = draw_boxes(draw_img, hot_windows, color=(0, 0, 255), thick=6) \n",
    "    \n",
    "#     _, box_list = find_cars(img, ystart, ystop, scale, svc, X_scaler, orient, pix_per_cell, cell_per_block, spatial_size, hist_bins)\n",
    "    # Read in image similar to one shown above \n",
    "    heat = np.zeros_like(img[:,:,0]).astype(np.float)\n",
    "\n",
    "    # Add heat to each box in box list\n",
    "    heat = add_heat(heat,hot_windows)\n",
    "\n",
    "    # Apply threshold to help remove false positives, this is for the cumulative threshold for all the recent detections\n",
    "    # only apply this if the frame passed is greater than the maximum size of the bb class\n",
    "    \n",
    "    # if this is used for image detection, use a threshold of 1\n",
    "    if bb == None:\n",
    "        heat = apply_threshold(heat, 0)\n",
    "    # else, use the maximum capacity of bbox as the threshold ( which means at least 1 detection per frame )\n",
    "    elif len(bb.bboxes) < bb.max_frame:\n",
    "        heat = apply_threshold(heat, len(bb.bboxes))\n",
    "    else:\n",
    "        heat = apply_threshold(heat, bb.max_frame)\n",
    "\n",
    "    # Visualize the heatmap when displaying    \n",
    "    heatmap = np.clip(heat, 0, 255)\n",
    "\n",
    "    # Find final boxes from heatmap using label function\n",
    "    labels = label(heatmap)\n",
    "    draw_img = draw_labeled_bboxes(np.copy(img), labels)\n",
    "    \n",
    "\n",
    "    \n",
    "    return draw_img , heatmap, window_img, this_detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detecting Cars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17760\n"
     ]
    }
   ],
   "source": [
    "# Read in cars and notcars\n",
    "cars = glob.glob('vehicles/*/*.png')\n",
    "notcars = glob.glob('non-vehicles/*/*.png')\n",
    "\n",
    "# Reduce the sample size because\n",
    "# The quiz evaluator times out after 13s of CPU time\n",
    "# sample_size = 500\n",
    "# cars = cars[0:sample_size]\n",
    "# notcars = notcars[0:sample_size]\n",
    "print(len(cars)+len(notcars))\n",
    "\n",
    "color_space = 'YCrCb' # Can be RGB, HSV, LUV, HLS, YUV, YCrCb\n",
    "orient = 9  # HOG orientations\n",
    "pix_per_cell = 8 # HOG pixels per cell\n",
    "cell_per_block = 2 # HOG cells per block\n",
    "hog_channel = 0 # Can be 0, 1, 2, or \"ALL\"\n",
    "spatial_size = (32, 32) # Spatial binning dimensions\n",
    "hist_bins = 32    # Number of histogram bins\n",
    "spatial_feat = True # Spatial features on or off\n",
    "hist_feat = True # Histogram features on or off\n",
    "hog_feat = True # HOG features on or off\n",
    "y_start_stop = [400, 600] # Min and max in y to search in slide_window()\n",
    "y_start_stop_up = [400, 450] # Min and max in y to search in slide_window() for upper half\n",
    "y_start_stop_down = [450, 600] # Min and max in y to search in slide_window() for lower half"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using: 9 orientations 8 pixels per cell and 2 cells per block\n",
      "Feature vector length: 4932\n",
      "15.81 Seconds to train SVC...\n",
      "Test Accuracy of SVC =  0.9735\n"
     ]
    }
   ],
   "source": [
    "# Train the classifier\n",
    "car_features = extract_features(cars, color_space=color_space, \n",
    "                        spatial_size=spatial_size, hist_bins=hist_bins, \n",
    "                        orient=orient, pix_per_cell=pix_per_cell, \n",
    "                        cell_per_block=cell_per_block, \n",
    "                        hog_channel=hog_channel, spatial_feat=spatial_feat, \n",
    "                        hist_feat=hist_feat, hog_feat=hog_feat)\n",
    "notcar_features = extract_features(notcars, color_space=color_space, \n",
    "                        spatial_size=spatial_size, hist_bins=hist_bins, \n",
    "                        orient=orient, pix_per_cell=pix_per_cell, \n",
    "                        cell_per_block=cell_per_block, \n",
    "                        hog_channel=hog_channel, spatial_feat=spatial_feat, \n",
    "                        hist_feat=hist_feat, hog_feat=hog_feat)\n",
    "\n",
    "X = np.vstack((car_features, notcar_features)).astype(np.float64)                        \n",
    "# Fit a per-column scaler\n",
    "X_scaler = StandardScaler().fit(X)\n",
    "# Apply the scaler to X\n",
    "scaled_X = X_scaler.transform(X)\n",
    "\n",
    "# Define the labels vector\n",
    "y = np.hstack((np.ones(len(car_features)), np.zeros(len(notcar_features))))\n",
    "\n",
    "\n",
    "# Split up data into randomized training and test sets\n",
    "rand_state = np.random.randint(0, 100)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    scaled_X, y, test_size=0.2, random_state=rand_state)\n",
    "\n",
    "print('Using:',orient,'orientations',pix_per_cell,\n",
    "    'pixels per cell and', cell_per_block,'cells per block')\n",
    "print('Feature vector length:', len(X_train[0]))\n",
    "# Use a linear SVC \n",
    "svc = LinearSVC()\n",
    "# Check the training time for the SVC\n",
    "t=time.time()\n",
    "svc.fit(X_train, y_train)\n",
    "t2 = time.time()\n",
    "print(round(t2-t, 2), 'Seconds to train SVC...')\n",
    "# Check the score of the SVC\n",
    "print('Test Accuracy of SVC = ', round(svc.score(X_test, y_test), 4))\n",
    "# Check the prediction time for a single sample\n",
    "t=time.time()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detect Lanes and Cars in Videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MoviePy] >>>> Building video output_video.mp4\n",
      "[MoviePy] Writing video output_video.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 1260/1261 [04:49<00:00,  4.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MoviePy] Done.\n",
      "[MoviePy] >>>> Video ready: output_video.mp4 \n",
      "\n",
      "CPU times: user 5min 14s, sys: 58.8 s, total: 6min 13s\n",
      "Wall time: 4min 49s\n"
     ]
    }
   ],
   "source": [
    "# Import everything needed to edit/save/watch video clips\n",
    "from moviepy.editor import VideoFileClip\n",
    "# Load the video\n",
    "yellow_output = 'output_video.mp4'\n",
    "## To speed up the testing process you may want to try your pipeline on a shorter subclip of the video\n",
    "## To do so add .subclip(start_second,end_second) to the end of the line below\n",
    "## Where start_second and end_second are integer values representing the start and end of the subclip\n",
    "## You may also uncomment the following line for a subclip of the first 5 seconds\n",
    "##clip2 = VideoFileClip('test_videos/solidYellowLeft.mp4').subclip(0,5)\n",
    "clip2 = VideoFileClip('project_video.mp4')\n",
    "yellow_clip = clip2.fl_image(detect_lanes_in_image)\n",
    "%time yellow_clip.write_videofile(yellow_output, audio=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_cars_all_in_one(img):\n",
    "    img_with_lanes = detect_lanes_in_image(img)\n",
    "    \n",
    "    result, _, _, current = find_cars_in_image(img, bb)\n",
    "    bb.add_bboxes(current)\n",
    "    \n",
    "    return (result * 255 + img_with_lanes) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MoviePy] >>>> Building video output_video_with_cars.mp4\n",
      "[MoviePy] Writing video output_video_with_cars.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 1260/1261 [13:43<00:00,  1.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MoviePy] Done.\n",
      "[MoviePy] >>>> Video ready: output_video_with_cars.mp4 \n",
      "\n",
      "CPU times: user 13min 40s, sys: 1min 29s, total: 15min 9s\n",
      "Wall time: 13min 44s\n"
     ]
    }
   ],
   "source": [
    "# Import everything needed to edit/save/watch video clips\n",
    "from moviepy.editor import VideoFileClip\n",
    "\n",
    "# Load the video\n",
    "yellow_output = 'output_video_with_cars.mp4'\n",
    "## To speed up the testing process you may want to try your pipeline on a shorter subclip of the video\n",
    "## To do so add .subclip(start_second,end_second) to the end of the line below\n",
    "## Where start_second and end_second are integer values representing the start and end of the subclip\n",
    "## You may also uncomment the following line for a subclip of the first 5 seconds\n",
    "##clip2 = VideoFileClip('test_videos/solidYellowLeft.mp4').subclip(0,5)\n",
    "clip2 = VideoFileClip('project_video.mp4')\n",
    "bb = BoundingBoxes(30) # use 10 for detecting opposite lane, 30 for lanes driving in the same direction\n",
    "yellow_clip = clip2.fl_image(find_cars_all_in_one)\n",
    "%time yellow_clip.write_videofile(yellow_output, audio=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
